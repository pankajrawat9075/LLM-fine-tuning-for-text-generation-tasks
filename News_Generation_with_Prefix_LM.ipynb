{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pankajrawat9075/Question-Answering-with-LLMs/blob/main/Prefix_Text_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## distinctions between prefixLM and Auto-Regressor LM\n",
        "\n",
        "The key distinction between prefix (conditional) LM and autoregressive LM lies in how they generate tokens. Prefix LM conditions token generation on explicit context information provided before each token, while autoregressive language modeling relies solely on the sequence of tokens generated up to that point.\n",
        "\n",
        "Prefix LM uses a prefix which can guide the generation process. This prefix can be an embedded image, title, context or anything that can be embedded and provided to the language model. So not only the LM generates next words based on previous words in context but also guided by this special prefix. This prefix is what makes it different from autoregressive model.\n",
        "\n",
        "This prefix when used with prefix fine-tuning methods make the training process much more efficient than the full autoregressive approach which is computationally heavy."
      ],
      "metadata": {
        "id": "wAornKifIfup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "()"
      ],
      "metadata": {
        "id": "kULv8Wm1oOSe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSWYe3LMSP-b"
      },
      "source": [
        "### Install and import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xk9lb9GmZuS"
      },
      "source": [
        "%%capture\n",
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IL6-XP7zzH7h"
      },
      "source": [
        "import os\n",
        "import io\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import zipfile\n",
        "import random\n",
        "import time\n",
        "import csv\n",
        "import datetime\n",
        "from itertools import compress\n",
        "from collections import Counter, defaultdict\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "from transformers import AutoTokenizer, AutoConfig, AutoModelForPreTraining, \\\n",
        "                         AdamW, get_linear_schedule_with_warmup, \\\n",
        "                         TrainingArguments, BeamScorer, Trainer\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, random_split, DataLoader, \\\n",
        "                             RandomSampler, SequentialSampler\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZDy9RClRiQ3"
      },
      "source": [
        "### Configurations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QILzrXuoRhaF"
      },
      "source": [
        "DEBUG           = False\n",
        "\n",
        "INPUT_DIR       = 'articles'\n",
        "\n",
        "MODEL           = 'gpt2' #{gpt2, gpt2-medium, gpt2-large, gpt2-xl}\n",
        "\n",
        "UNFREEZE_LAST_N = 6 #The last N layers to unfreeze for training\n",
        "\n",
        "SPECIAL_TOKENS  = { \"bos_token\": \"<|BOS|>\",\n",
        "                    \"eos_token\": \"<|EOS|>\",\n",
        "                    \"unk_token\": \"<|UNK|>\",\n",
        "                    \"pad_token\": \"<|PAD|>\",\n",
        "                    \"sep_token\": \"<|SEP|>\"}\n",
        "\n",
        "MAXLEN          = 768  #{768, 1024, 1280, 1600}\n",
        "\n",
        "TRAIN_SIZE      = 0.8\n",
        "\n",
        "TRAIN_BATCHSIZE = 2\n",
        "BATCH_UPDATE    = 32\n",
        "\n",
        "EPOCHS          = 4\n",
        "LR              = 5e-4\n",
        "EPS             = 1e-8\n",
        "WARMUP_STEPS    = 1e2\n",
        "\n",
        "SEED            = 2020"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "740ZIyZXRbWe"
      },
      "source": [
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "seed_everything(SEED)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RpxktDOHpMI"
      },
      "source": [
        "### Download News Aggregator Dataset\n",
        "\n",
        "https://archive.ics.uci.edu/ml/datasets/News+Aggregator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDqrrihRHrVH"
      },
      "source": [
        "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00359/NewsAggregatorDataset.zip'\n",
        "r = requests.get(url)\n",
        "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "z.extractall()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "id": "tsFp19NmHv64",
        "outputId": "aa7d08a5-8d85-48a7-bb6f-a65616caf6c8"
      },
      "source": [
        "columns = ['ID',\n",
        "           'TITLE',\n",
        "           'URL',\n",
        "           'PUBLISHER',\n",
        "           'CATEGORY', #News category (b = business, t = science and technology, e = entertainment, m = health)\n",
        "           'Alphanumeric ID',\n",
        "           'HOSTNAME Url',\n",
        "           'TIMESTAMP']\n",
        "\n",
        "df = pd.read_csv(\"newsCorpora.csv\", sep='\\t', header=None, names=columns)\n",
        "print(f\"df size: {len(df) :,}\")\n",
        "\n",
        "df.head(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "df size: 422,419\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   ID                                              TITLE  \\\n",
              "0   1  Fed official says weak data caused by weather,...   \n",
              "1   2  Fed's Charles Plosser sees high bar for change...   \n",
              "\n",
              "                                                 URL          PUBLISHER  \\\n",
              "0  http://www.latimes.com/business/money/la-fi-mo...  Los Angeles Times   \n",
              "1  http://www.livemint.com/Politics/H2EvwJSK2VE6O...           Livemint   \n",
              "\n",
              "  CATEGORY                Alphanumeric ID      HOSTNAME Url      TIMESTAMP  \n",
              "0        b  ddUyU0VZz0BRneMioxUPQVP6sIxvM   www.latimes.com  1394470370698  \n",
              "1        b  ddUyU0VZz0BRneMioxUPQVP6sIxvM  www.livemint.com  1394470371207  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-07d8545d-9cff-4384-b547-6cd2f0e62466\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>TITLE</th>\n",
              "      <th>URL</th>\n",
              "      <th>PUBLISHER</th>\n",
              "      <th>CATEGORY</th>\n",
              "      <th>Alphanumeric ID</th>\n",
              "      <th>HOSTNAME Url</th>\n",
              "      <th>TIMESTAMP</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Fed official says weak data caused by weather,...</td>\n",
              "      <td>http://www.latimes.com/business/money/la-fi-mo...</td>\n",
              "      <td>Los Angeles Times</td>\n",
              "      <td>b</td>\n",
              "      <td>ddUyU0VZz0BRneMioxUPQVP6sIxvM</td>\n",
              "      <td>www.latimes.com</td>\n",
              "      <td>1394470370698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Fed's Charles Plosser sees high bar for change...</td>\n",
              "      <td>http://www.livemint.com/Politics/H2EvwJSK2VE6O...</td>\n",
              "      <td>Livemint</td>\n",
              "      <td>b</td>\n",
              "      <td>ddUyU0VZz0BRneMioxUPQVP6sIxvM</td>\n",
              "      <td>www.livemint.com</td>\n",
              "      <td>1394470371207</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-07d8545d-9cff-4384-b547-6cd2f0e62466')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-07d8545d-9cff-4384-b547-6cd2f0e62466 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-07d8545d-9cff-4384-b547-6cd2f0e62466');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-8b0e6de7-cd5f-43f5-b53c-ee58c610dbc0\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8b0e6de7-cd5f-43f5-b53c-ee58c610dbc0')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-8b0e6de7-cd5f-43f5-b53c-ee58c610dbc0 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHuvpmHFSrer"
      },
      "source": [
        "### Download articles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhSFcfg4_kzY",
        "outputId": "81c3cbac-e28d-489b-e728-7ef9ef4d764f"
      },
      "source": [
        "#Download news articles\n",
        "!gdown --id 1FqqDZOVd8_LfOEA_2DEY5W70-DpQVF-F\n",
        "!unzip -q '/content/news_articles.zip'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gdown/cli.py:138: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1FqqDZOVd8_LfOEA_2DEY5W70-DpQVF-F\n",
            "From (redirected): https://drive.google.com/uc?id=1FqqDZOVd8_LfOEA_2DEY5W70-DpQVF-F&confirm=t&uuid=a7f5c1c3-1d85-40a7-8443-bf775d99cc09\n",
            "To: /content/news_articles.zip\n",
            "100% 49.2M/49.2M [00:00<00:00, 229MB/s]\n",
            "replace articles/100019.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace articles/100022.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bha20KNAGL11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cdc96f2-e133-4b99-b7fb-27799d31c562"
      },
      "source": [
        "%%time\n",
        "\n",
        "data = dict()\n",
        "for root, dirs, files in os.walk(INPUT_DIR, topdown=True):\n",
        "    t0 = time.time()\n",
        "\n",
        "    for i, f in enumerate(files):\n",
        "        #id, category, title, keywords, text\n",
        "        id = int(f[:-4])\n",
        "        tmp = df[['CATEGORY', 'TITLE']][df.ID==id].values\n",
        "        category, title = tmp[0][0], tmp[0][1]\n",
        "\n",
        "        with open(f'{INPUT_DIR}/{f}', \"r\") as infile:\n",
        "            text = infile.read()\n",
        "\n",
        "        data[id] = [title, text]\n",
        "\n",
        "        if i%1000==0 and i>0:\n",
        "            clear_output(wait=True)\n",
        "            print(f\"({os.getpid()}) Items processed: {i :,}/{len(files):,}; {(time.time()-t0)/60 :.1f} minutes\")\n",
        "\n",
        "            if DEBUG:\n",
        "                break\n",
        "\n",
        "print(f\"Number of articles: {len(data) :,}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5565) Items processed: 39,000/39,024; 9.0 minutes\n",
            "Number of articles: 39,024\n",
            "CPU times: user 8min 39s, sys: 3.96 s, total: 8min 43s\n",
            "Wall time: 9min\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1La_j8ZXoV2K"
      },
      "source": [
        "### Download Keywords\n",
        "Keywords of these articles are available in github"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTQq3OkzeaCG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51b77446-4c3d-4a65-87a3-9c0bc05ef791"
      },
      "source": [
        "#Download Keywords\n",
        "!gdown --id 1C1WWvnt2egzhRmVXMSJhARz5GOLDGzMC\n",
        "!unzip -q '/content/keywords.csv.zip'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gdown/cli.py:138: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1C1WWvnt2egzhRmVXMSJhARz5GOLDGzMC\n",
            "To: /content/keywords.csv.zip\n",
            "100% 746k/746k [00:00<00:00, 37.8MB/s]\n",
            "replace keywords.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MNo-mjNhEWx"
      },
      "source": [
        "def read_keywords():\n",
        "    keywords = dict()\n",
        "    with open('keywords.csv', newline='') as f:\n",
        "        reader = csv.reader(f, delimiter=',')\n",
        "        for row in reader:\n",
        "            keywords[int(row[0])] = row[1:]\n",
        "    print(f\"Number of entries in keywords: {len(keywords) :,}\")\n",
        "    return keywords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvxg8Q1_WtVl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e81210db-152e-4c62-ee47-92942a92076e"
      },
      "source": [
        "%%time\n",
        "keywords = read_keywords()\n",
        "\n",
        "all_keywords = set()\n",
        "for k, v in keywords.items():\n",
        "    for w in v:\n",
        "        all_keywords.add(w)\n",
        "\n",
        "for id in data.keys():\n",
        "    data[id].append(keywords[id])\n",
        "\n",
        "print(f\"Number of unique keywords: {len(all_keywords) :,}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of entries in keywords: 39,024\n",
            "Number of unique keywords: 29,291\n",
            "CPU times: user 132 ms, sys: 11 ms, total: 143 ms\n",
            "Wall time: 144 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2j0V83HbH6QF"
      },
      "source": [
        "### Datasets and loaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8gp0I8JnMEE"
      },
      "source": [
        "class myDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data, tokenizer, randomize=True):\n",
        "\n",
        "        title, text, keywords = [], [], []\n",
        "        for k, v in data.items():\n",
        "            title.append(v[0])\n",
        "            text.append(v[1])\n",
        "            keywords.append(v[2])\n",
        "\n",
        "        self.randomize = randomize\n",
        "        self.tokenizer = tokenizer\n",
        "        self.title     = title\n",
        "        self.text      = text\n",
        "        self.keywords  = keywords\n",
        "\n",
        "    #---------------------------------------------#\n",
        "\n",
        "    @staticmethod\n",
        "    def join_keywords(keywords, randomize=True):\n",
        "        N = len(keywords)\n",
        "\n",
        "        #random sampling and shuffle\n",
        "        if randomize:\n",
        "            M = random.choice(range(N+1))\n",
        "            keywords = keywords[:M]\n",
        "            random.shuffle(keywords)\n",
        "\n",
        "        return ','.join(keywords)\n",
        "\n",
        "    #---------------------------------------------#\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "\n",
        "    #---------------------------------------------#\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        keywords = self.keywords[i].copy()\n",
        "        kw = self.join_keywords(keywords, self.randomize)\n",
        "\n",
        "        input = SPECIAL_TOKENS['bos_token'] + self.title[i] + \\\n",
        "                SPECIAL_TOKENS['sep_token'] + kw + SPECIAL_TOKENS['sep_token'] + \\\n",
        "                self.text[i] + SPECIAL_TOKENS['eos_token']\n",
        "\n",
        "        encodings_dict = tokenizer(input,\n",
        "                                   truncation=True,\n",
        "                                   max_length=MAXLEN,\n",
        "                                   padding=\"max_length\")\n",
        "\n",
        "        input_ids = encodings_dict['input_ids']\n",
        "        attention_mask = encodings_dict['attention_mask']\n",
        "\n",
        "        return {'label': torch.tensor(input_ids),\n",
        "                'input_ids': torch.tensor(input_ids),\n",
        "                'attention_mask': torch.tensor(attention_mask)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwCXZgyrU7P5"
      },
      "source": [
        "def split_data(data, S=TRAIN_SIZE):\n",
        "    # Shuffle ids\n",
        "    ids = list(data.keys())\n",
        "    random.shuffle(ids)\n",
        "\n",
        "    # Split into training and validation sets\n",
        "    train_size = int(S * len(data))\n",
        "\n",
        "    train_ids = ids[:train_size]\n",
        "    val_ids = ids[train_size:]\n",
        "\n",
        "    train_data = dict()\n",
        "    for id in train_ids:\n",
        "        train_data[id] = data[id]\n",
        "\n",
        "    val_data = dict()\n",
        "    for id in val_ids:\n",
        "        val_data[id] = data[id]\n",
        "\n",
        "    return train_data, val_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3LfEbc5j9Yo"
      },
      "source": [
        "### Loading Tokenizer, Config and Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knL24TEIX9fl"
      },
      "source": [
        "def get_tokenier(special_tokens=None):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL) #GPT2Tokenizer\n",
        "\n",
        "    if special_tokens:\n",
        "        tokenizer.add_special_tokens(special_tokens)\n",
        "        print(\"Special tokens added\")\n",
        "    return tokenizer\n",
        "\n",
        "def get_model(tokenizer, special_tokens=None, load_model_path=None):\n",
        "\n",
        "    #GPT2LMHeadModel\n",
        "    if special_tokens:\n",
        "        config = AutoConfig.from_pretrained(MODEL,\n",
        "                                            bos_token_id=tokenizer.bos_token_id,\n",
        "                                            eos_token_id=tokenizer.eos_token_id,\n",
        "                                            sep_token_id=tokenizer.sep_token_id,\n",
        "                                            pad_token_id=tokenizer.pad_token_id,\n",
        "                                            output_hidden_states=False)\n",
        "    else:\n",
        "        config = AutoConfig.from_pretrained(MODEL,\n",
        "                                            pad_token_id=tokenizer.eos_token_id,\n",
        "                                            output_hidden_states=False)\n",
        "\n",
        "    #----------------------------------------------------------------#\n",
        "    model = AutoModelForPreTraining.from_pretrained(MODEL, config=config)\n",
        "\n",
        "    if special_tokens:\n",
        "        #Special tokens added, model needs to be resized accordingly\n",
        "        model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "    if load_model_path:\n",
        "        model.load_state_dict(torch.load(load_model_path))\n",
        "\n",
        "    model.cuda()\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2zrELuFTzEG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc1328a9-003d-44c5-c1fe-00efc781d12c"
      },
      "source": [
        "%%time\n",
        "\n",
        "tokenizer = get_tokenier(special_tokens=SPECIAL_TOKENS)\n",
        "model = get_model(tokenizer,\n",
        "                  special_tokens=SPECIAL_TOKENS,\n",
        "                #   load_model_path='pytorch_model.bin'\n",
        "                 )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Special tokens added\n",
            "CPU times: user 2.14 s, sys: 760 ms, total: 2.9 s\n",
            "Wall time: 4.6 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iGa-FRWAzWI"
      },
      "source": [
        "# - Freeze selective layers:\n",
        "# - Freeze all layers except last n:\n",
        "for parameter in model.parameters():\n",
        "    parameter.requires_grad = False\n",
        "\n",
        "for i, m in enumerate(model.transformer.h):\n",
        "    #Only un-freeze the last n transformer blocks\n",
        "    if i+1 > 12 - UNFREEZE_LAST_N:\n",
        "        for parameter in m.parameters():\n",
        "            parameter.requires_grad = True\n",
        "\n",
        "for parameter in model.transformer.ln_f.parameters():\n",
        "    parameter.requires_grad = True\n",
        "\n",
        "for parameter in model.lm_head.parameters():\n",
        "    parameter.requires_grad = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHYTARXZOKwP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4b1d19b7-97d0-4de7-f2b9-28e5ed639faf"
      },
      "source": [
        "train_data, val_data = split_data(data)\n",
        "\n",
        "train_dataset = myDataset(train_data, tokenizer)\n",
        "val_dataset = myDataset(val_data, tokenizer, randomize=False)\n",
        "\n",
        "f'There are {len(train_dataset) :,} samples for training, and {len(val_dataset) :,} samples for validation testing'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'There are 31,219 samples for training, and 7,805 samples for validation testing'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYh9gAM_lAxK"
      },
      "source": [
        "### Fine-tune GPT2 using Trainer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate -U transformers[torch]\n"
      ],
      "metadata": {
        "id": "n0yC9hQtBY8f",
        "outputId": "fd4827f7-7499-4a5f-884c-d87d723b9b85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 149
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Successfully uninstalled transformers-4.35.2\n",
            "Successfully installed transformers-4.37.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "transformers"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsKQJis8jcCh"
      },
      "source": [
        "%%time\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/\",\n",
        "    num_train_epochs=EPOCHS,\n",
        "    per_device_train_batch_size=TRAIN_BATCHSIZE,\n",
        "    per_device_eval_batch_size=TRAIN_BATCHSIZE,\n",
        "    gradient_accumulation_steps=BATCH_UPDATE,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    fp16=True,\n",
        "    fp16_opt_level=APEX_OPT_LEVEL,\n",
        "    warmup_steps=WARMUP_STEPS,\n",
        "    learning_rate=LR,\n",
        "    adam_epsilon=EPS,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=1,\n",
        "    load_best_model_at_end=True,\n",
        "    save_strategy=\"epoch\"\n",
        ")\n",
        "\n",
        "#---------------------------------------------------#\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "#---------------------------------------------------#\n",
        "trainer.train()\n",
        "trainer.save_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "eval_results = trainer.evaluate()\n",
        "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
      ],
      "metadata": {
        "id": "ORSX-aiuEkp8",
        "outputId": "db73ccd9-05f4-4a52-c1c6-86928faffc4b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Perplexity: 13.43\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0azvVPXCx4eM"
      },
      "source": [
        "### Generating text with Fine-tuned GPT-2 model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dojGngEDRupX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "beea7b67-8385-40c7-d347-a736fd393949"
      },
      "source": [
        "tokenizer = get_tokenier(special_tokens=SPECIAL_TOKENS)\n",
        "model = get_model(tokenizer,\n",
        "                  special_tokens=SPECIAL_TOKENS,\n",
        "                  load_model_path='pytorch_model.bin')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Special tokens added\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYCC-ugJJy3A"
      },
      "source": [
        "title = \"We got a lot of grief when our photo became a meme\"\n",
        "keywords = ['train', 'lads', 'drinking', 'picture', 'funny', 'instagram']\n",
        "kw = myDataset.join_keywords(keywords, randomize=False)\n",
        "\n",
        "prompt = SPECIAL_TOKENS['bos_token'] + title + \\\n",
        "         SPECIAL_TOKENS['sep_token'] + kw + SPECIAL_TOKENS['sep_token']\n",
        "\n",
        "generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
        "device = torch.device(\"cuda\")\n",
        "generated = generated.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1gM2DvGeh2i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e600a352-2cae-477a-8b94-cbc0ef6dcb44"
      },
      "source": [
        "# Top-p (nucleus) text generation (10 samples):\n",
        "sample_outputs = model.generate(generated,\n",
        "                                do_sample=True,\n",
        "                                min_length=50,\n",
        "                                max_length=MAXLEN,\n",
        "                                top_k=30,\n",
        "                                top_p=0.7,\n",
        "                                temperature=0.9,\n",
        "                                repetition_penalty=2.0,\n",
        "                                num_return_sequences=10\n",
        "                                )\n",
        "\n",
        "for i, sample_output in enumerate(sample_outputs):\n",
        "    text = tokenizer.decode(sample_output, skip_special_tokens=True)\n",
        "    a = len(title) + len(','.join(keywords))\n",
        "    print(\"{}: {}\\n\\n\".format(i+1,  text[a:]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1: It's been an amazing ride so far. It’s just such incredible people that made it to the top and now we have something bigger than anyone could ever dream for!\n",
            "Now everyone has had their share in this fun moment – from famous rockers like The Rolling Stones on down below… or celebrities who donned costumes (or simply posed as they did at concerts). Here are some highlights:\n",
            "\n",
            "The train was full-on hilarious after all — but not by accident - here is how one Twitter user posted her own reaction pic...\n",
            "\n",
            "\n",
            "2: It was fun. It wasn't really funny but it didn’t hurt us at all and we did appreciate the fact that people were laughing out loud about how much they enjoyed what I put on Instagram as well!\n",
            "The pictures are from last week in which Lindsay Lohan showed off her new drink while driving to work with an Italian businessman named Giannelli Vittorio (who has been linked up by The Sun). There is also this picture taken during one trip back home for my birthday where she had dinner alongside other men who have worked together – apparently both before he died earlier today…so there you go:\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "3: It was like being in love with someone who died. It wasn't just me and my friends that were sad; it's all the pictures we posted online.\"\n",
            "\n",
            "\n",
            "4: Unfortunately for the majority (more than half) it was taken down soon after.\n",
            "The picture has since been deleted from all accounts and there are no plans to do any more with this image now that we have found its way back online...\n",
            "\n",
            "\n",
            "5: Get all the very latest news in Ireland straight to your email every single day Sign up! Thank you for subscribing We have more newsletters Show me See its privacy notice Invalid Email\n",
            "Video Loading Video Unavailable Click To play Tap TO GO The video will auto-play soon 8 Cancel Play now\n",
            "\n",
            " A picture shared on Facebook shows two men smoking an orange joint and joking about “hippie”. One was carrying around with him several bottles while another carried out some sort Of Polio vaccination. And there were also pictures taken showing one man getting into trouble after he posted his alleged mugshot online - this time it is from L'Wren Scott (who has been linked by Twitter users) who had recently hit back at people tweeting photos she thought weren't funny or meant any harm.. There are loads we can take away but here's just four: \"I'm sorry if I look like that,\" said Mr Stacey, posing alongside her as they smoked what appeared be ordinary cigarettes which looked similar enough... He then proceeded later telling them how disgusting their behaviour would make women feel.\"The boys told us not too long ago these guys did drink beer\" during sex so why wouldn`t anyone want something bad behind closed doors? Then again maybe someone didnâ€¦d think twice before posting pics... What do YOU THINK??\n",
            "\n",
            "\n",
            "6: I thought it was just some sad old-fashioned way to send out an image. But now we’re talking about this: the pictures that are on every train stop in London have been taken down and replaced with ones from other cities all over Britain as part Of The Funniest Place To Live For Summer 2014 (with photos).\n",
            "The first picture is one posted by Twitter user @_dickman123 which shows us two young girls dancing around Central Park together – apparently without their parents' approval! This means you can see them enjoying themselves while behind closed doors at your local cafe or restaurant… well not really quite so much because they actually don't seem like real adults anymore but hey presto…. what happened?\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "7: Our first picture was taken by an 18-year old girl who is now on Instagram. It shows her sitting in front with the caption \"I just want to say thank you for all your support.\"\n",
            "But that image has been viewed more than 10 million times since it appeared and shared nearly 3 billion time as we’ve written about how great those messages are from people like this one:\n",
            "\n",
            "\n",
            "8: Our website was inundated with hundreds and thousands. But we wanted to make sure everyone had the opportunity not only as an online poster but also in real life - so that they would have access at all times during their daily lives.\"\n",
            "\n",
            "\n",
            "9: Our dear friends at the Boston Globe are mourning over what happened to one person's life.\n",
            "The story goes that we were trying too hard and didn't get anything done about it: “Someone was killed in an accident on this train last night (sic)” wrote Chris Martin via Twitter as he sat down for his daily dose Of Grief-baiting News Feeds — after which I posted photos from my own trip across town showing how terrible things have become since then! It wasnâ€¦t all bad but not everyone is so forgiving… We donÂve been blessed with such amazing lives today & will cherish them forever – @ChrisMartin pic.(BostonGlobe.) June 15th 2014\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "10: It's time for an update on the horrible train accident that took place in London.\n",
            "The picture has now gone viral and everyone is talking about how terrible it was to be out there partying with your friends while drinking beer together at one point!\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAmwMuxa3xGW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "429ec8c0-4c8a-481a-e001-a944c734be49"
      },
      "source": [
        "# Beam-search text generation:\n",
        "sample_outputs = model.generate(generated,\n",
        "                                do_sample=True,\n",
        "                                max_length=MAXLEN,\n",
        "                                num_beams=5,\n",
        "                                repetition_penalty=5.0,\n",
        "                                early_stopping=True,\n",
        "                                num_return_sequences=1\n",
        "                                )\n",
        "\n",
        "for i, sample_output in enumerate(sample_outputs):\n",
        "    text = tokenizer.decode(sample_output, skip_special_tokens=True)\n",
        "    a = len(title) + len(','.join(keywords))\n",
        "    print(\"{}: {}\\n\\n\".format(i+1,  text[a:]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1: The world’s most popular dating app crashed on its iOS and Android operating systems this week.\n",
            "\n",
            "It has since been taken down from Apple’s App Store after users complained that the picture was too close to their faces. The company said in a statement: “This is not an isolated incident. We take great pride in what we do and look forward to working with others to make the sharing experience even better.”\n",
            "\n",
            "Scroll down for video\n",
            "\n",
            "\n",
            "\n",
            "Our hearts go out to all those who have lost their lives trying to find love over the past few days - but sadly it looks like there may be more to this tragic story than meets the eye\n",
            "\n",
            "No-one knows exactly how many people were affected by the crash, which took place between late February and early March last year (stock image)\n",
            "\n",
            "HOW IT WORKS A simple message will appear on your iPhone or iPad screen telling you if you are having trouble finding someone online. It then lets you meet up with them so they can pick you up where you left off. In other words, as soon as you hit the 'Find My Friends' button at the bottom of the screen, you become friends again. If you don’t get matching messages within 24 hours, you lose access to any kind of contact information such as email, phone numbers, dates of birth etc.\n",
            "\n",
            "INDY/ LIFE Newsletter Be inspired with the latest lifestyle trends every week Please enter your email address Please enter a valid email address Please enter a valid email address SIGN UP Thanks for signing up to the INDY/LIFE newsletter {{#verifyErrors}} {{message}} {{/verifyErrors}} {{^verifyErrors}} {{message}} {{/verifyErrors}} The Independent would like to keep you informed about offers, events and updates by email, please tick the box if you would like to be contacted\n",
            "\n",
            "Read our full mailing list consent terms here INDY/ LIFE Newsletter Be inspired with the latest lifestyle trends every week The Independent would like to keep you informed about offers, events and updates by email, please tick the box if you would like to be contacted\n",
            "\n",
            "Read our full mailing list consent terms here\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4UjMTwWx-ky"
      },
      "source": [
        "### Generating text with raw GPT2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbiaQldb1RPO"
      },
      "source": [
        "tokenizer = get_tokenier()\n",
        "model = get_model(tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1ag9Z0iZbzG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1aab1e57-e34c-4988-9575-994d790c03cc"
      },
      "source": [
        "prompt = title\n",
        "\n",
        "generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
        "device = torch.device(\"cuda\")\n",
        "generated = generated.to(device)\n",
        "\n",
        "model.eval()\n",
        "sample_outputs = model.generate(generated,\n",
        "                                do_sample=True,\n",
        "                                max_length=MAXLEN,\n",
        "                                num_beams=5,\n",
        "                                repetition_penalty=5.0,\n",
        "                                early_stopping=True,\n",
        "                                num_return_sequences=1\n",
        "                                )\n",
        "\n",
        "for i, sample_output in enumerate(sample_outputs):\n",
        "    print(\"{}: {}\\n\\n\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0: We got a lot of grief when our photo became a meme.\n",
            "\n",
            "\"I didn't know what to do with it,\" she said. \"It was just so much fun.\"\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
