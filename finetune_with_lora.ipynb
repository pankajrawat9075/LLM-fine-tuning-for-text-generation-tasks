{
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "abb08a7a07b6478a857ee79bf3aaeff2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e9da5c876d6e4aea996ca2dc16831884",
              "IPY_MODEL_d5293e9772b5415d950a2db51b881816"
            ],
            "layout": "IPY_MODEL_440971181243446296f2fd31d4148fb8"
          }
        },
        "e9da5c876d6e4aea996ca2dc16831884": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c6e72f4e242841ada43ef90f12f3a2f5",
            "placeholder": "​",
            "style": "IPY_MODEL_48a2c369fcf04e49872824dd21be5c7b",
            "value": "0.020 MB of 0.020 MB uploaded\r"
          }
        },
        "d5293e9772b5415d950a2db51b881816": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6af2d28ba93440cba0f13c1738b53edf",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c6489dd9887c42baad309c2456323d0a",
            "value": 1
          }
        },
        "440971181243446296f2fd31d4148fb8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6e72f4e242841ada43ef90f12f3a2f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48a2c369fcf04e49872824dd21be5c7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6af2d28ba93440cba0f13c1738b53edf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6489dd9887c42baad309c2456323d0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30648,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pankajrawat9075/Question-Answering-with-LLMs/blob/main/finetune_with_lora.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Install Dependencies"
      ],
      "metadata": {
        "id": "e2v4gKKvmMmt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pyarrow==11.0.0 bitsandbytes datasets accelerate loralib peft transformers wandb nvidia-ml-py3\n"
      ],
      "metadata": {
        "id": "EKhSxsxjZZkf",
        "execution": {
          "iopub.status.busy": "2024-02-19T08:19:37.083398Z",
          "iopub.execute_input": "2024-02-19T08:19:37.083738Z",
          "iopub.status.idle": "2024-02-19T08:19:55.922187Z",
          "shell.execute_reply.started": "2024-02-19T08:19:37.083712Z",
          "shell.execute_reply": "2024-02-19T08:19:55.921078Z"
        },
        "trusted": true
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### check if cuda available"
      ],
      "metadata": {
        "id": "WEl3slpuGWM9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovdVG8UAYt1c",
        "outputId": "0320182c-9bbb-447c-de9f-b2ca0d57da8f",
        "execution": {
          "iopub.status.busy": "2024-02-19T06:19:44.760113Z",
          "iopub.execute_input": "2024-02-19T06:19:44.760520Z",
          "iopub.status.idle": "2024-02-19T06:19:48.257257Z",
          "shell.execute_reply.started": "2024-02-19T06:19:44.760481Z",
          "shell.execute_reply": "2024-02-19T06:19:48.255889Z"
        },
        "trusted": true
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### few helper functions"
      ],
      "metadata": {
        "id": "fE9Fvc0Vkz6k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pynvml import *\n",
        "gpu_details={}\n",
        "\n",
        "def print_gpu_utilization():\n",
        "    nvmlInit()\n",
        "    handle = nvmlDeviceGetHandleByIndex(0)\n",
        "    info = nvmlDeviceGetMemoryInfo(handle)\n",
        "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
        "    return {info.used//1024**2}\n",
        "\n",
        "\n",
        "def print_summary(result, name='default'):\n",
        "    print(f\"Time: {result.metrics['train_runtime']:.2f}\")\n",
        "    print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\")\n",
        "    gpu_details[name] = {'s/s': result.metrics['train_samples_per_second'], 'gpu': print_gpu_utilization()}"
      ],
      "metadata": {
        "id": "DI5DyizdkyxA"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_details['initial']= {'gpu': print_gpu_utilization()}"
      ],
      "metadata": {
        "id": "6gB8GKGhlpU_",
        "outputId": "6e628afa-3ee3-4940-aab0-ececcdd9e963",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU memory occupied: 260 MB.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_details"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXF_KM-EvT7m",
        "outputId": "ef46e667-07bd-4097-81d0-e76a155473ba"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'initial': {'gpu': {260}}}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### import tokenizer and model"
      ],
      "metadata": {
        "id": "VxETFOt_GWM-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import bitsandbytes as bnb\n",
        "\n",
        "# uncomment the following to load bloom model\n",
        "\n",
        "##\n",
        "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"bigscience/bloom-560m\",\n",
        "    device_map='auto',\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/tokenizer\")\n",
        "##\n",
        "\n",
        "# comment the following if want to use bloom model\n",
        "\n",
        "##\n",
        "# from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "# tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
        "# model = GPT2LMHeadModel.from_pretrained('distilgpt2')\n",
        "##\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-z4V86S1Y1e_",
        "outputId": "bd618721-b705-4aa8-d979-b6b8d1516e88",
        "execution": {
          "iopub.status.busy": "2024-02-19T08:23:17.074150Z",
          "iopub.execute_input": "2024-02-19T08:23:17.074571Z",
          "iopub.status.idle": "2024-02-19T08:23:29.018739Z",
          "shell.execute_reply.started": "2024-02-19T08:23:17.074536Z",
          "shell.execute_reply": "2024-02-19T08:23:29.017625Z"
        },
        "trusted": true
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_gpu_utilization()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xOArUyenyJv",
        "outputId": "626fc660-7f39-44e5-f8c3-5df4f013015c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU memory occupied: 2494 MB.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{2494}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# add padding token\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "1wtMvegDKiaD",
        "execution": {
          "iopub.status.busy": "2024-02-19T09:26:42.322746Z",
          "iopub.execute_input": "2024-02-19T09:26:42.323104Z",
          "iopub.status.idle": "2024-02-19T09:26:42.329074Z",
          "shell.execute_reply.started": "2024-02-19T09:26:42.323075Z",
          "shell.execute_reply": "2024-02-19T09:26:42.328088Z"
        },
        "trusted": true
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lW_jGitDrkjN"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Model before adding LORA Adapter \\n\")\n",
        "print(\"no. of parameters : \", model.num_parameters())\n",
        "print(\"\\n\",model)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-19T08:29:07.015439Z",
          "iopub.execute_input": "2024-02-19T08:29:07.015846Z",
          "iopub.status.idle": "2024-02-19T08:29:07.021950Z",
          "shell.execute_reply.started": "2024-02-19T08:29:07.015818Z",
          "shell.execute_reply": "2024-02-19T08:29:07.020963Z"
        },
        "trusted": true,
        "id": "Dm5l09UbGWNB",
        "outputId": "7320fa58-7bcf-4e2e-a7ca-870da0a4ebcc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model before adding LORA Adapter \n",
            "\n",
            "no. of parameters :  559214592\n",
            "\n",
            " BloomForCausalLM(\n",
            "  (transformer): BloomModel(\n",
            "    (word_embeddings): Embedding(250880, 1024)\n",
            "    (word_embeddings_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "    (h): ModuleList(\n",
            "      (0-23): 24 x BloomBlock(\n",
            "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (self_attention): BloomAttention(\n",
            "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
            "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): BloomMLP(\n",
            "          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          (gelu_impl): BloomGelu()\n",
            "          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=1024, out_features=250880, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### freeze the pretrained model"
      ],
      "metadata": {
        "id": "bvycuc-GGWNC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print_gpu_utilization()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJOpgPIJwsiJ",
        "outputId": "068a785f-c1b2-4283-ea83-e057a0ee510f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU memory occupied: 2494 MB.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{2494}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for param in model.parameters():\n",
        "  param.requires_grad = True  # freeze the model - train adapters later"
      ],
      "metadata": {
        "id": "TynZHYQuZK7X",
        "execution": {
          "iopub.status.busy": "2024-02-19T08:33:47.564545Z",
          "iopub.execute_input": "2024-02-19T08:33:47.565376Z",
          "iopub.status.idle": "2024-02-19T08:33:47.570531Z",
          "shell.execute_reply.started": "2024-02-19T08:33:47.565345Z",
          "shell.execute_reply": "2024-02-19T08:33:47.569570Z"
        },
        "trusted": true
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Helper function to print parameters"
      ],
      "metadata": {
        "id": "U_aocRwDGWND"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )"
      ],
      "metadata": {
        "id": "5cN2kJXhZZQ9",
        "execution": {
          "iopub.status.busy": "2024-02-19T08:34:02.504012Z",
          "iopub.execute_input": "2024-02-19T08:34:02.504354Z",
          "iopub.status.idle": "2024-02-19T08:34:02.510384Z",
          "shell.execute_reply.started": "2024-02-19T08:34:02.504330Z",
          "shell.execute_reply": "2024-02-19T08:34:02.509430Z"
        },
        "trusted": true
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### add lora adapter to the model"
      ],
      "metadata": {
        "id": "n-cEyc3iGWNE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=2,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"query_key_value\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model_lora = get_peft_model(model, config)\n",
        "print_trainable_parameters(model_lora)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-YV0erfZcCi",
        "outputId": "acd3b97b-6296-4566-a179-9d7567c1cb14",
        "execution": {
          "iopub.status.busy": "2024-02-19T08:35:46.948194Z",
          "iopub.execute_input": "2024-02-19T08:35:46.949122Z",
          "iopub.status.idle": "2024-02-19T08:35:47.166028Z",
          "shell.execute_reply.started": "2024-02-19T08:35:46.949090Z",
          "shell.execute_reply": "2024-02-19T08:35:47.165044Z"
        },
        "trusted": true
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 196608 || all params: 559411200 || trainable%: 0.03514552443712246\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see remarkable decrease in the no. of parameters to train than if we would have trained the whole model. We now only train 0.24% of actual parameters. WOW!!"
      ],
      "metadata": {
        "id": "Uc4dPFVgGWNF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in model_lora.named_parameters():\n",
        "    print(f\"Parameter {name} requires gradients: {param.requires_grad}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1HHVQKq-4krP",
        "outputId": "b3ef5f79-1425-42f9-ced6-8ab2f6f9a514"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter base_model.model.transformer.word_embeddings.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.word_embeddings_layernorm.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.word_embeddings_layernorm.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.0.input_layernorm.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.0.input_layernorm.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.0.self_attention.query_key_value.base_layer.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.0.self_attention.query_key_value.base_layer.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.0.self_attention.query_key_value.lora_A.default.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.0.self_attention.query_key_value.lora_B.default.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.0.self_attention.dense.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.0.self_attention.dense.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.0.post_attention_layernorm.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.0.post_attention_layernorm.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.0.mlp.dense_h_to_4h.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.0.mlp.dense_h_to_4h.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.0.mlp.dense_4h_to_h.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.0.mlp.dense_4h_to_h.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.1.input_layernorm.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.1.input_layernorm.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.1.self_attention.query_key_value.base_layer.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.1.self_attention.query_key_value.base_layer.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.1.self_attention.query_key_value.lora_A.default.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.1.self_attention.query_key_value.lora_B.default.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.1.self_attention.dense.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.1.self_attention.dense.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.1.post_attention_layernorm.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.1.post_attention_layernorm.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.1.mlp.dense_h_to_4h.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.1.mlp.dense_h_to_4h.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.1.mlp.dense_4h_to_h.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.1.mlp.dense_4h_to_h.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.2.input_layernorm.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.2.input_layernorm.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.2.self_attention.query_key_value.base_layer.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.2.self_attention.query_key_value.base_layer.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.2.self_attention.query_key_value.lora_A.default.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.2.self_attention.query_key_value.lora_B.default.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.2.self_attention.dense.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.2.self_attention.dense.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.2.post_attention_layernorm.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.2.post_attention_layernorm.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.2.mlp.dense_h_to_4h.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.2.mlp.dense_h_to_4h.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.2.mlp.dense_4h_to_h.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.2.mlp.dense_4h_to_h.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.3.input_layernorm.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.3.input_layernorm.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.3.self_attention.query_key_value.base_layer.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.3.self_attention.query_key_value.base_layer.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.3.self_attention.query_key_value.lora_A.default.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.3.self_attention.query_key_value.lora_B.default.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.3.self_attention.dense.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.3.self_attention.dense.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.3.post_attention_layernorm.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.3.post_attention_layernorm.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.3.mlp.dense_h_to_4h.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.3.mlp.dense_h_to_4h.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.3.mlp.dense_4h_to_h.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.3.mlp.dense_4h_to_h.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.4.input_layernorm.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.4.input_layernorm.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.4.self_attention.query_key_value.base_layer.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.4.self_attention.query_key_value.base_layer.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.4.self_attention.query_key_value.lora_A.default.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.4.self_attention.query_key_value.lora_B.default.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.4.self_attention.dense.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.4.self_attention.dense.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.4.post_attention_layernorm.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.4.post_attention_layernorm.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.4.mlp.dense_h_to_4h.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.4.mlp.dense_h_to_4h.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.4.mlp.dense_4h_to_h.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.4.mlp.dense_4h_to_h.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.5.input_layernorm.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.5.input_layernorm.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.5.self_attention.query_key_value.base_layer.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.5.self_attention.query_key_value.base_layer.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.5.self_attention.query_key_value.lora_A.default.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.5.self_attention.query_key_value.lora_B.default.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.5.self_attention.dense.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.5.self_attention.dense.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.5.post_attention_layernorm.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.5.post_attention_layernorm.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.5.mlp.dense_h_to_4h.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.5.mlp.dense_h_to_4h.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.5.mlp.dense_4h_to_h.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.5.mlp.dense_4h_to_h.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.6.input_layernorm.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.6.input_layernorm.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.6.self_attention.query_key_value.base_layer.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.6.self_attention.query_key_value.base_layer.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.6.self_attention.query_key_value.lora_A.default.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.6.self_attention.query_key_value.lora_B.default.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.6.self_attention.dense.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.6.self_attention.dense.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.6.post_attention_layernorm.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.6.post_attention_layernorm.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.6.mlp.dense_h_to_4h.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.6.mlp.dense_h_to_4h.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.6.mlp.dense_4h_to_h.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.6.mlp.dense_4h_to_h.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.7.input_layernorm.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.7.input_layernorm.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.7.self_attention.query_key_value.base_layer.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.7.self_attention.query_key_value.base_layer.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.7.self_attention.query_key_value.lora_A.default.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.7.self_attention.query_key_value.lora_B.default.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.7.self_attention.dense.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.7.self_attention.dense.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.7.post_attention_layernorm.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.7.post_attention_layernorm.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.7.mlp.dense_h_to_4h.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.7.mlp.dense_h_to_4h.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.7.mlp.dense_4h_to_h.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.7.mlp.dense_4h_to_h.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.8.input_layernorm.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.8.input_layernorm.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.8.self_attention.query_key_value.base_layer.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.8.self_attention.query_key_value.base_layer.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.8.self_attention.query_key_value.lora_A.default.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.8.self_attention.query_key_value.lora_B.default.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.8.self_attention.dense.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.8.self_attention.dense.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.8.post_attention_layernorm.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.8.post_attention_layernorm.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.8.mlp.dense_h_to_4h.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.8.mlp.dense_h_to_4h.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.8.mlp.dense_4h_to_h.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.8.mlp.dense_4h_to_h.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.9.input_layernorm.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.9.input_layernorm.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.9.self_attention.query_key_value.base_layer.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.9.self_attention.query_key_value.base_layer.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.9.self_attention.query_key_value.lora_A.default.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.9.self_attention.query_key_value.lora_B.default.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.9.self_attention.dense.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.9.self_attention.dense.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.9.post_attention_layernorm.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.9.post_attention_layernorm.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.9.mlp.dense_h_to_4h.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.9.mlp.dense_h_to_4h.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.9.mlp.dense_4h_to_h.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.9.mlp.dense_4h_to_h.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.10.input_layernorm.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.10.input_layernorm.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.10.self_attention.query_key_value.base_layer.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.10.self_attention.query_key_value.base_layer.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.10.self_attention.query_key_value.lora_A.default.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.10.self_attention.query_key_value.lora_B.default.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.10.self_attention.dense.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.10.self_attention.dense.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.10.post_attention_layernorm.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.10.post_attention_layernorm.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.10.mlp.dense_h_to_4h.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.10.mlp.dense_h_to_4h.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.10.mlp.dense_4h_to_h.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.10.mlp.dense_4h_to_h.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.11.input_layernorm.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.11.input_layernorm.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.11.self_attention.query_key_value.base_layer.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.11.self_attention.query_key_value.base_layer.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.11.self_attention.query_key_value.lora_A.default.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.11.self_attention.query_key_value.lora_B.default.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.11.self_attention.dense.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.11.self_attention.dense.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.11.post_attention_layernorm.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.11.post_attention_layernorm.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.11.mlp.dense_h_to_4h.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.11.mlp.dense_h_to_4h.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.11.mlp.dense_4h_to_h.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.11.mlp.dense_4h_to_h.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.12.input_layernorm.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.12.input_layernorm.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.12.self_attention.query_key_value.base_layer.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.12.self_attention.query_key_value.base_layer.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.12.self_attention.query_key_value.lora_A.default.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.12.self_attention.query_key_value.lora_B.default.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.12.self_attention.dense.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.12.self_attention.dense.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.12.post_attention_layernorm.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.12.post_attention_layernorm.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.12.mlp.dense_h_to_4h.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.12.mlp.dense_h_to_4h.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.12.mlp.dense_4h_to_h.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.12.mlp.dense_4h_to_h.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.13.input_layernorm.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.13.input_layernorm.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.13.self_attention.query_key_value.base_layer.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.13.self_attention.query_key_value.base_layer.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.13.self_attention.query_key_value.lora_A.default.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.13.self_attention.query_key_value.lora_B.default.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.13.self_attention.dense.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.13.self_attention.dense.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.13.post_attention_layernorm.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.13.post_attention_layernorm.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.13.mlp.dense_h_to_4h.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.13.mlp.dense_h_to_4h.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.13.mlp.dense_4h_to_h.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.13.mlp.dense_4h_to_h.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.14.input_layernorm.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.14.input_layernorm.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.14.self_attention.query_key_value.base_layer.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.14.self_attention.query_key_value.base_layer.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.14.self_attention.query_key_value.lora_A.default.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.14.self_attention.query_key_value.lora_B.default.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.14.self_attention.dense.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.14.self_attention.dense.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.14.post_attention_layernorm.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.14.post_attention_layernorm.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.14.mlp.dense_h_to_4h.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.14.mlp.dense_h_to_4h.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.14.mlp.dense_4h_to_h.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.14.mlp.dense_4h_to_h.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.15.input_layernorm.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.15.input_layernorm.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.15.self_attention.query_key_value.base_layer.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.15.self_attention.query_key_value.base_layer.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.15.self_attention.query_key_value.lora_A.default.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.15.self_attention.query_key_value.lora_B.default.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.15.self_attention.dense.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.15.self_attention.dense.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.15.post_attention_layernorm.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.15.post_attention_layernorm.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.15.mlp.dense_h_to_4h.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.15.mlp.dense_h_to_4h.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.15.mlp.dense_4h_to_h.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.15.mlp.dense_4h_to_h.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.16.input_layernorm.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.16.input_layernorm.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.16.self_attention.query_key_value.base_layer.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.16.self_attention.query_key_value.base_layer.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.16.self_attention.query_key_value.lora_A.default.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.16.self_attention.query_key_value.lora_B.default.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.16.self_attention.dense.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.16.self_attention.dense.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.16.post_attention_layernorm.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.16.post_attention_layernorm.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.16.mlp.dense_h_to_4h.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.16.mlp.dense_h_to_4h.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.16.mlp.dense_4h_to_h.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.16.mlp.dense_4h_to_h.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.17.input_layernorm.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.17.input_layernorm.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.17.self_attention.query_key_value.base_layer.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.17.self_attention.query_key_value.base_layer.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.17.self_attention.query_key_value.lora_A.default.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.17.self_attention.query_key_value.lora_B.default.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.17.self_attention.dense.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.17.self_attention.dense.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.17.post_attention_layernorm.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.17.post_attention_layernorm.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.17.mlp.dense_h_to_4h.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.17.mlp.dense_h_to_4h.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.17.mlp.dense_4h_to_h.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.17.mlp.dense_4h_to_h.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.18.input_layernorm.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.18.input_layernorm.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.18.self_attention.query_key_value.base_layer.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.18.self_attention.query_key_value.base_layer.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.18.self_attention.query_key_value.lora_A.default.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.18.self_attention.query_key_value.lora_B.default.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.18.self_attention.dense.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.18.self_attention.dense.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.18.post_attention_layernorm.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.18.post_attention_layernorm.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.18.mlp.dense_h_to_4h.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.18.mlp.dense_h_to_4h.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.18.mlp.dense_4h_to_h.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.18.mlp.dense_4h_to_h.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.19.input_layernorm.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.19.input_layernorm.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.19.self_attention.query_key_value.base_layer.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.19.self_attention.query_key_value.base_layer.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.19.self_attention.query_key_value.lora_A.default.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.19.self_attention.query_key_value.lora_B.default.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.19.self_attention.dense.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.19.self_attention.dense.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.19.post_attention_layernorm.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.19.post_attention_layernorm.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.19.mlp.dense_h_to_4h.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.19.mlp.dense_h_to_4h.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.19.mlp.dense_4h_to_h.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.19.mlp.dense_4h_to_h.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.20.input_layernorm.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.20.input_layernorm.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.20.self_attention.query_key_value.base_layer.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.20.self_attention.query_key_value.base_layer.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.20.self_attention.query_key_value.lora_A.default.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.20.self_attention.query_key_value.lora_B.default.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.20.self_attention.dense.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.20.self_attention.dense.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.20.post_attention_layernorm.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.20.post_attention_layernorm.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.20.mlp.dense_h_to_4h.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.20.mlp.dense_h_to_4h.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.20.mlp.dense_4h_to_h.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.20.mlp.dense_4h_to_h.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.21.input_layernorm.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.21.input_layernorm.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.21.self_attention.query_key_value.base_layer.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.21.self_attention.query_key_value.base_layer.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.21.self_attention.query_key_value.lora_A.default.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.21.self_attention.query_key_value.lora_B.default.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.21.self_attention.dense.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.21.self_attention.dense.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.21.post_attention_layernorm.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.21.post_attention_layernorm.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.21.mlp.dense_h_to_4h.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.21.mlp.dense_h_to_4h.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.21.mlp.dense_4h_to_h.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.21.mlp.dense_4h_to_h.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.22.input_layernorm.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.22.input_layernorm.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.22.self_attention.query_key_value.base_layer.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.22.self_attention.query_key_value.base_layer.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.22.self_attention.query_key_value.lora_A.default.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.22.self_attention.query_key_value.lora_B.default.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.22.self_attention.dense.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.22.self_attention.dense.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.22.post_attention_layernorm.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.22.post_attention_layernorm.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.22.mlp.dense_h_to_4h.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.22.mlp.dense_h_to_4h.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.22.mlp.dense_4h_to_h.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.22.mlp.dense_4h_to_h.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.23.input_layernorm.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.23.input_layernorm.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.23.self_attention.query_key_value.base_layer.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.23.self_attention.query_key_value.base_layer.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.23.self_attention.query_key_value.lora_A.default.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.23.self_attention.query_key_value.lora_B.default.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.23.self_attention.dense.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.23.self_attention.dense.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.23.post_attention_layernorm.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.23.post_attention_layernorm.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.23.mlp.dense_h_to_4h.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.23.mlp.dense_h_to_4h.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.h.23.mlp.dense_4h_to_h.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.h.23.mlp.dense_4h_to_h.bias requires gradients: True\n",
            "Parameter base_model.model.transformer.ln_f.weight requires gradients: True\n",
            "Parameter base_model.model.transformer.ln_f.bias requires gradients: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Model after adding LORA Adapter\\n\")\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcydVpi2AH3v",
        "outputId": "1bb0e280-b600-4a90-f376-68d0af732514",
        "execution": {
          "iopub.status.busy": "2024-02-19T08:37:10.794152Z",
          "iopub.execute_input": "2024-02-19T08:37:10.795062Z",
          "iopub.status.idle": "2024-02-19T08:37:10.801794Z",
          "shell.execute_reply.started": "2024-02-19T08:37:10.795028Z",
          "shell.execute_reply": "2024-02-19T08:37:10.800745Z"
        },
        "trusted": true
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model after adding LORA Adapter\n",
            "\n",
            "BloomForCausalLM(\n",
            "  (transformer): BloomModel(\n",
            "    (word_embeddings): Embedding(250880, 1024)\n",
            "    (word_embeddings_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "    (h): ModuleList(\n",
            "      (0-23): 24 x BloomBlock(\n",
            "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (self_attention): BloomAttention(\n",
            "          (query_key_value): lora.Linear(\n",
            "            (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
            "            (lora_dropout): ModuleDict(\n",
            "              (default): Dropout(p=0.05, inplace=False)\n",
            "            )\n",
            "            (lora_A): ModuleDict(\n",
            "              (default): Linear(in_features=1024, out_features=2, bias=False)\n",
            "            )\n",
            "            (lora_B): ModuleDict(\n",
            "              (default): Linear(in_features=2, out_features=3072, bias=False)\n",
            "            )\n",
            "            (lora_embedding_A): ParameterDict()\n",
            "            (lora_embedding_B): ParameterDict()\n",
            "          )\n",
            "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): BloomMLP(\n",
            "          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          (gelu_impl): BloomGelu()\n",
            "          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=1024, out_features=250880, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_details['model']= {'gpu': print_gpu_utilization()}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvjythXeoDOL",
        "outputId": "f675a3ca-c3f4-4663-e724-b9aa9d6f1f6c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU memory occupied: 2494 MB.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_details"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptoUX07wyfoU",
        "outputId": "325ebedd-0cf2-49f2-c62a-4c80c47d9259"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'initial': {'gpu': {260}}, 'model': {'gpu': {2494}}}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### load and prepare the dataset for training"
      ],
      "metadata": {
        "id": "Sm-2jTL5GWNG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "qa_dataset = load_dataset(\"squad_v2\")"
      ],
      "metadata": {
        "id": "QhpGk7bAZflY",
        "execution": {
          "iopub.status.busy": "2024-02-19T08:38:02.909789Z",
          "iopub.execute_input": "2024-02-19T08:38:02.910171Z",
          "iopub.status.idle": "2024-02-19T08:38:24.113811Z",
          "shell.execute_reply.started": "2024-02-19T08:38:02.910142Z",
          "shell.execute_reply": "2024-02-19T08:38:24.112877Z"
        },
        "trusted": true
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_dataset"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-19T08:38:41.039082Z",
          "iopub.execute_input": "2024-02-19T08:38:41.040019Z",
          "iopub.status.idle": "2024-02-19T08:38:41.046354Z",
          "shell.execute_reply.started": "2024-02-19T08:38:41.039985Z",
          "shell.execute_reply": "2024-02-19T08:38:41.045402Z"
        },
        "trusted": true,
        "id": "LtSsF63SGWNH",
        "outputId": "22d16aea-a73b-43b2-9e13-89ea7fbb7461",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "        num_rows: 130319\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "        num_rows: 11873\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### let's see some examples of our dataset"
      ],
      "metadata": {
        "id": "QQq1b6DsGWNI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qa_dataset['train'][0]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-19T08:39:28.205662Z",
          "iopub.execute_input": "2024-02-19T08:39:28.206031Z",
          "iopub.status.idle": "2024-02-19T08:39:28.214861Z",
          "shell.execute_reply.started": "2024-02-19T08:39:28.206004Z",
          "shell.execute_reply": "2024-02-19T08:39:28.213951Z"
        },
        "trusted": true,
        "id": "eo_qLlcvGWNJ",
        "outputId": "94bdb4a5-11eb-4201-9cb8-9a3e62e2ca0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': '56be85543aeaaa14008c9063',\n",
              " 'title': 'Beyoncé',\n",
              " 'context': 'Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".',\n",
              " 'question': 'When did Beyonce start becoming popular?',\n",
              " 'answers': {'text': ['in the late 1990s'], 'answer_start': [269]}}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So basically, we will be given a context and a question, and using the context itself - the model needs to return the answer."
      ],
      "metadata": {
        "id": "0AI74oNVGWNJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### select subset of data for finetuneing"
      ],
      "metadata": {
        "id": "PotVdsslGWNJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qa_dataset_train = qa_dataset[\"train\"].shuffle(seed=42).select(range(20000))\n",
        "qa_dataset_test = qa_dataset[\"validation\"].shuffle(seed=42).select(range(100))"
      ],
      "metadata": {
        "id": "4L3SvZMoCl8a",
        "execution": {
          "iopub.status.busy": "2024-02-19T08:41:21.914741Z",
          "iopub.execute_input": "2024-02-19T08:41:21.915758Z",
          "iopub.status.idle": "2024-02-19T08:41:21.986691Z",
          "shell.execute_reply.started": "2024-02-19T08:41:21.915720Z",
          "shell.execute_reply": "2024-02-19T08:41:21.985960Z"
        },
        "trusted": true
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_dataset_train"
      ],
      "metadata": {
        "id": "NkLH-gpoCgQh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "191e95aa-7277-4fca-a921-b74f5942b51b",
        "execution": {
          "iopub.status.busy": "2024-02-19T08:41:30.605000Z",
          "iopub.execute_input": "2024-02-19T08:41:30.605857Z",
          "iopub.status.idle": "2024-02-19T08:41:30.611655Z",
          "shell.execute_reply.started": "2024-02-19T08:41:30.605824Z",
          "shell.execute_reply": "2024-02-19T08:41:30.610658Z"
        },
        "trusted": true
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "    num_rows: 20000\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want our prompt to looks like this.\n",
        "```\n",
        "### CONTEXT\n",
        "{context}\n",
        "\n",
        "### QUESTION\n",
        "{question}\n",
        "\n",
        "### ANSWER\n",
        "{answer}</s>\n",
        "```"
      ],
      "metadata": {
        "id": "WkSImv-FGWNK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_prompt(context, question, answer):\n",
        "  if len(answer[\"text\"]) < 1:\n",
        "    answer = \"Cannot Find Answer\"\n",
        "  else:\n",
        "    answer = answer[\"text\"][0]\n",
        "  prompt_template = f\"### CONTEXT\\n{context}\\n\\n### QUESTION\\n{question}\\n\\n### ANSWER\\n{answer}</s>\"\n",
        "  return prompt_template\n",
        "\n",
        "mapped_qa_dataset_train = qa_dataset_train.map(lambda samples: tokenizer(create_prompt(samples['context'], samples['question'], samples['answers'])))\n",
        "mapped_qa_dataset_test = qa_dataset_test.map(lambda samples: tokenizer(create_prompt(samples['context'], samples['question'], samples['answers'])))"
      ],
      "metadata": {
        "id": "Uynj2xF6Zjc7",
        "execution": {
          "iopub.status.busy": "2024-02-19T08:43:15.610481Z",
          "iopub.execute_input": "2024-02-19T08:43:15.611196Z",
          "iopub.status.idle": "2024-02-19T08:43:30.021935Z",
          "shell.execute_reply.started": "2024-02-19T08:43:15.611161Z",
          "shell.execute_reply": "2024-02-19T08:43:30.021141Z"
        },
        "trusted": true
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mapped_qa_dataset_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ko4gtHEWDLQM",
        "outputId": "e8e2bc36-ce42-45ec-b1fc-1ef1234d2f8c",
        "execution": {
          "iopub.status.busy": "2024-02-19T08:43:50.403370Z",
          "iopub.execute_input": "2024-02-19T08:43:50.404119Z",
          "iopub.status.idle": "2024-02-19T08:43:50.410334Z",
          "shell.execute_reply.started": "2024-02-19T08:43:50.404088Z",
          "shell.execute_reply": "2024-02-19T08:43:50.409189Z"
        },
        "trusted": true
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['id', 'title', 'context', 'question', 'answers', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
              "    num_rows: 20000\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have got the dataset into trainable format using tokenizer"
      ],
      "metadata": {
        "id": "Q0dGlf6qGWNL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### using wandb for logging purposes"
      ],
      "metadata": {
        "id": "qKezb1YnGWNL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb"
      ],
      "metadata": {
        "id": "BFAeSdK4FDH3",
        "execution": {
          "iopub.status.busy": "2024-02-19T06:27:00.135448Z",
          "iopub.execute_input": "2024-02-19T06:27:00.135843Z",
          "iopub.status.idle": "2024-02-19T06:27:01.051106Z",
          "shell.execute_reply.started": "2024-02-19T06:27:00.135815Z",
          "shell.execute_reply": "2024-02-19T06:27:01.050240Z"
        },
        "trusted": true
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "api_key = 'c891ce08bb56081ecfa97de464a131634657ac13'\n",
        "wandb.login()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-19T06:27:13.328416Z",
          "iopub.execute_input": "2024-02-19T06:27:13.329147Z",
          "iopub.status.idle": "2024-02-19T06:27:58.128171Z",
          "shell.execute_reply.started": "2024-02-19T06:27:13.329111Z",
          "shell.execute_reply": "2024-02-19T06:27:58.126943Z"
        },
        "trusted": true,
        "id": "u-yCeRk9GWNL",
        "outputId": "21896cfb-edbd-4e56-81f3-ef99718f8bc5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs22m062\u001b[0m (\u001b[33miitmadras\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's log every trained model.\n",
        "%env WANDB_LOG_MODEL=true"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-19T06:28:39.609504Z",
          "iopub.execute_input": "2024-02-19T06:28:39.610215Z",
          "iopub.status.idle": "2024-02-19T06:28:39.616273Z",
          "shell.execute_reply.started": "2024-02-19T06:28:39.610180Z",
          "shell.execute_reply": "2024-02-19T06:28:39.615266Z"
        },
        "trusted": true,
        "id": "31B1g1VPGWNM",
        "outputId": "5bbba024-8b40-421a-d6e6-e433bcf5ebbd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: WANDB_LOG_MODEL=true\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"WANDB_PROJECT\"]=\"fine-tuning-with-LORA\""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-19T06:34:15.976001Z",
          "iopub.execute_input": "2024-02-19T06:34:15.976948Z",
          "iopub.status.idle": "2024-02-19T06:34:15.981369Z",
          "shell.execute_reply.started": "2024-02-19T06:34:15.976913Z",
          "shell.execute_reply": "2024-02-19T06:34:15.980346Z"
        },
        "trusted": true,
        "id": "zai26BOqGWNM"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ze6w59NRpbWB",
        "outputId": "a815fac2-bea5-41ac-f574-4b14e24535eb"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Mar 14 16:55:14 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   49C    P0              27W /  70W |   3333MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_gpu_utilization()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwSbjjrdzcSk",
        "outputId": "87777b10-ba56-4df7-9dfb-88e249e742ae"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU memory occupied: 3590 MB.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{3590}"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.finish(\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69,
          "referenced_widgets": [
            "abb08a7a07b6478a857ee79bf3aaeff2",
            "e9da5c876d6e4aea996ca2dc16831884",
            "d5293e9772b5415d950a2db51b881816",
            "440971181243446296f2fd31d4148fb8",
            "c6e72f4e242841ada43ef90f12f3a2f5",
            "48a2c369fcf04e49872824dd21be5c7b",
            "6af2d28ba93440cba0f13c1738b53edf",
            "c6489dd9887c42baad309c2456323d0a"
          ]
        },
        "id": "6w4KxFiBzG9x",
        "outputId": "3e1d49d9-22c8-4b95-e73f-296fede838e4"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "abb08a7a07b6478a857ee79bf3aaeff2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">lora-r-4-grad-cp</strong> at: <a href='https://wandb.ai/iitmadras/fine-tuning-with-LORA/runs/qshqa8cb' target=\"_blank\">https://wandb.ai/iitmadras/fine-tuning-with-LORA/runs/qshqa8cb</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20240314_164956-qshqa8cb/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "\n",
        "# use only when we want to train / fine-tune the model\n",
        "trainer = transformers.Trainer(\n",
        "    model=model_lora,\n",
        "    train_dataset=mapped_qa_dataset_train,\n",
        "    eval_dataset=mapped_qa_dataset_test,\n",
        "    args=transformers.TrainingArguments(\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=1,\n",
        "        gradient_checkpointing=True,\n",
        "        warmup_steps=100,\n",
        "        max_steps=10,\n",
        "        learning_rate=1e-3,\n",
        "        fp16=False,\n",
        "        logging_steps=2,\n",
        "        output_dir='outputs',     # we save the model after training for testing pusposes\n",
        "        evaluation_strategy='steps',\n",
        "        report_to=\"wandb\",\n",
        "        run_name=\"lora-r-4-grad-cp\",\n",
        "    ),\n",
        "#     compute_metrics=compute_perplexity,\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        ")\n",
        "# model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
        "result = trainer.train()\n"
      ],
      "metadata": {
        "id": "9s3hMLqrZoBo",
        "execution": {
          "iopub.status.busy": "2024-02-19T08:55:42.080866Z",
          "iopub.execute_input": "2024-02-19T08:55:42.081464Z",
          "iopub.status.idle": "2024-02-19T08:55:42.184153Z",
          "shell.execute_reply.started": "2024-02-19T08:55:42.081435Z",
          "shell.execute_reply": "2024-02-19T08:55:42.183429Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 628
        },
        "outputId": "8191ad1d-b956-4292-c7db-936f73dbfd1c"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 3/10 00:00 < 00:05, 1.17 it/s, Epoch 0.00/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 1/13 : < :]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 2.52 GiB. GPU 0 has a total capacity of 14.75 GiB of which 2.03 GiB is free. Process 239672 has 12.72 GiB memory in use. Of the allocated memory 11.82 GiB is allocated by PyTorch, and 785.97 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-3a3c0f8277f3>\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1622\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1623\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1624\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   1625\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1626\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2027\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_step_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2029\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_log_save_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2030\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2031\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_substep_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2410\u001b[0m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2411\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_evaluate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2412\u001b[0;31m             \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2413\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_report_to_hp_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3228\u001b[0m         \u001b[0meval_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_loop\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_legacy_prediction_loop\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3229\u001b[0;31m         output = eval_loop(\n\u001b[0m\u001b[1;32m   3230\u001b[0m             \u001b[0meval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3231\u001b[0m             \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Evaluation\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3417\u001b[0m             \u001b[0;31m# Prediction step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3418\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_loss_only\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3419\u001b[0m             \u001b[0mmain_input_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"main_input_name\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3420\u001b[0m             \u001b[0minputs_decode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmain_input_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minclude_inputs_for_metrics\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mprediction_step\u001b[0;34m(self, model, inputs, prediction_loss_only, ignore_keys)\u001b[0m\n\u001b[1;32m   3633\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mhas_labels\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mloss_without_labels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3634\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3635\u001b[0;31m                         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3636\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2923\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2924\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2925\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2926\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2927\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1089\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpeft_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpeft_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mPeftType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPOLY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"task_ids\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1091\u001b[0;31m             return self.base_model(\n\u001b[0m\u001b[1;32m   1092\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1093\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/tuners/tuners_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bloom/modeling_bloom.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, **deprecated_arguments)\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0;31m# Flatten the tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m             \u001b[0mloss_fct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m             loss = loss_fct(\n\u001b[0m\u001b[1;32m    884\u001b[0m                 \u001b[0mshift_logits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshift_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1179\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m   1180\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m                                label_smoothing=self.label_smoothing)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3057\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3058\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3059\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3061\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.52 GiB. GPU 0 has a total capacity of 14.75 GiB of which 2.03 GiB is free. Process 239672 has 12.72 GiB memory in use. Of the allocated memory 11.82 GiB is allocated by PyTorch, and 785.97 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_summary(result, 'vanila')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nE0v60g73LeC",
        "outputId": "902b1143-dd99-4b3b-ccee-54e757ce2ead"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time: 61.90\n",
            "Samples/second: 0.16\n",
            "GPU memory occupied: 14298 MB.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "OSKV65bU2SKQ"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%env PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D00QtP-bz10t",
        "outputId": "c7b461a2-ee65-4c79-dcad-e4d06957f317"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_summary(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "iP5wFDzPzoo4",
        "outputId": "5f04d586-ac9e-483c-d366-f9d2e5b7f255"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'result' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-8c526037715e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'result' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# already trained the model and saved it. Now can use it anytime\n",
        "model_lora = GPT2LMHeadModel.from_pretrained(\"/kaggle/working/outputs/checkpoint-500\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-19T08:52:38.180912Z",
          "iopub.execute_input": "2024-02-19T08:52:38.181290Z",
          "iopub.status.idle": "2024-02-19T08:52:39.213643Z",
          "shell.execute_reply.started": "2024-02-19T08:52:38.181257Z",
          "shell.execute_reply": "2024-02-19T08:52:39.212760Z"
        },
        "trusted": true,
        "id": "7M8oIXErGWNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "eval_results = trainer.evaluate()\n",
        "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-19T08:55:47.497413Z",
          "iopub.execute_input": "2024-02-19T08:55:47.498004Z",
          "iopub.status.idle": "2024-02-19T08:56:38.984465Z",
          "shell.execute_reply.started": "2024-02-19T08:55:47.497974Z",
          "shell.execute_reply": "2024-02-19T08:56:38.983621Z"
        },
        "trusted": true,
        "id": "pWIPW09rGWNM",
        "outputId": "f983e2c0-a943-48a4-d1d8-9aca256d8e46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 00:06]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:",
          "output_type": "stream"
        },
        {
          "output_type": "stream",
          "name": "stdin",
          "text": "  ········································\n"
        },
        {
          "name": "stderr",
          "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "wandb version 0.16.3 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Tracking run with wandb version 0.16.2"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Run data is saved locally in <code>/kaggle/working/wandb/run-20240219_085607-fvz95gsq</code>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Syncing run <strong><a href='https://wandb.ai/iitmadras/huggingface/runs/fvz95gsq' target=\"_blank\">lora-r=4</a></strong> to <a href='https://wandb.ai/iitmadras/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View project at <a href='https://wandb.ai/iitmadras/huggingface' target=\"_blank\">https://wandb.ai/iitmadras/huggingface</a>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View run at <a href='https://wandb.ai/iitmadras/huggingface/runs/fvz95gsq' target=\"_blank\">https://wandb.ai/iitmadras/huggingface/runs/fvz95gsq</a>"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": ">>> Perplexity: 23.54\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\">>> Evaluation_loss: {eval_results['eval_loss']}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-19T08:58:22.390881Z",
          "iopub.execute_input": "2024-02-19T08:58:22.391260Z",
          "iopub.status.idle": "2024-02-19T08:58:22.398738Z",
          "shell.execute_reply.started": "2024-02-19T08:58:22.391230Z",
          "shell.execute_reply": "2024-02-19T08:58:22.397809Z"
        },
        "trusted": true,
        "id": "HWayD5CEGWNN",
        "outputId": "30a10269-1c91-4cdc-93f1-1a52c89e85ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": ">>> Evaluation_loss: 3.158553123474121\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### save the model to hugging-face hub"
      ],
      "metadata": {
        "id": "j1OBvFcYGWNO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HUGGING_FACE_USER_NAME = \"pankaj9075rawat\""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-19T09:11:27.921972Z",
          "iopub.execute_input": "2024-02-19T09:11:27.923081Z",
          "iopub.status.idle": "2024-02-19T09:11:27.928885Z",
          "shell.execute_reply.started": "2024-02-19T09:11:27.923036Z",
          "shell.execute_reply": "2024-02-19T09:11:27.927796Z"
        },
        "trusted": true,
        "id": "whx5tq-xGWNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-19T09:11:32.500934Z",
          "iopub.execute_input": "2024-02-19T09:11:32.501868Z",
          "iopub.status.idle": "2024-02-19T09:11:32.530586Z",
          "shell.execute_reply.started": "2024-02-19T09:11:32.501830Z",
          "shell.execute_reply": "2024-02-19T09:11:32.529452Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "9e3596809ade4c1ea4ca5fff6811a500"
          ]
        },
        "id": "5_vOdt3VGWNP",
        "outputId": "c07b845e-acad-48d9-abaa-19c7da944745"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9e3596809ade4c1ea4ca5fff6811a500"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"fine-tune-GPT2-lora\"\n",
        "\n",
        "model_lora.push_to_hub(f\"{HUGGING_FACE_USER_NAME}/{model_name}\", use_auth_token=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-19T09:18:56.890400Z",
          "iopub.execute_input": "2024-02-19T09:18:56.891078Z",
          "iopub.status.idle": "2024-02-19T09:18:59.206545Z",
          "shell.execute_reply.started": "2024-02-19T09:18:56.891045Z",
          "shell.execute_reply": "2024-02-19T09:18:59.205513Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "06613f547e0148459ad4c22f32644738",
            "fe167e1a36104e57b0a0387405bb5ae6"
          ]
        },
        "id": "fVPMSf2VGWNP",
        "outputId": "9adcde37-6b77-4985-d11d-4bb72d476880"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:821: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "README.md:   0%|          | 0.00/5.18k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "06613f547e0148459ad4c22f32644738"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/transformers/integrations/peft.py:391: FutureWarning: The `active_adapter` method is deprecated and will be removed in a future version.\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "adapter_model.safetensors:   0%|          | 0.00/816k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fe167e1a36104e57b0a0387405bb5ae6"
            }
          },
          "metadata": {}
        },
        {
          "execution_count": 41,
          "output_type": "execute_result",
          "data": {
            "text/plain": "CommitInfo(commit_url='https://huggingface.co/pankaj9075rawat/fine-tune-GPT2-lora/commit/2a22914b9d9027edfcd65b474d1b48e6435471b9', commit_message='Upload model', commit_description='', oid='2a22914b9d9027edfcd65b474d1b48e6435471b9', pr_url=None, pr_revision=None, pr_num=None)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "def make_inference(context, question):\n",
        "  batch = tokenizer(f\"### CONTEXT\\n{context}\\n\\n### QUESTION\\n{question}\\n\\n### ANSWER\\n\", return_tensors='pt')\n",
        "\n",
        "  # Move the batch tensor to the appropriate device (e.g., GPU)\n",
        "  batch = {k: v.to(\"cuda\") for k, v in batch.items()}\n",
        "\n",
        "  with torch.cuda.amp.autocast():\n",
        "    output_tokens = model_lora.generate(**batch, max_new_tokens=500)\n",
        "\n",
        "  display(Markdown((tokenizer.decode(output_tokens[0], skip_special_tokens=True))))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-19T09:33:44.431364Z",
          "iopub.execute_input": "2024-02-19T09:33:44.432056Z",
          "iopub.status.idle": "2024-02-19T09:33:44.440056Z",
          "shell.execute_reply.started": "2024-02-19T09:33:44.432021Z",
          "shell.execute_reply": "2024-02-19T09:33:44.438953Z"
        },
        "trusted": true,
        "id": "6KLNL3ynGWNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_lora.config.use_cache = True"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-19T09:19:36.097355Z",
          "iopub.execute_input": "2024-02-19T09:19:36.098183Z",
          "iopub.status.idle": "2024-02-19T09:19:36.103127Z",
          "shell.execute_reply.started": "2024-02-19T09:19:36.098149Z",
          "shell.execute_reply": "2024-02-19T09:19:36.102164Z"
        },
        "trusted": true,
        "id": "GBjruDnxGWNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### make some inferences"
      ],
      "metadata": {
        "id": "ahl2KwB7GWNQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context = \"Cheese is the best food.\"\n",
        "question = \"What is the best food?\"\n",
        "\n",
        "make_inference(context, question)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-19T09:33:46.978676Z",
          "iopub.execute_input": "2024-02-19T09:33:46.979377Z",
          "iopub.status.idle": "2024-02-19T09:33:53.900583Z",
          "shell.execute_reply.started": "2024-02-19T09:33:46.979348Z",
          "shell.execute_reply": "2024-02-19T09:33:53.899526Z"
        },
        "trusted": true,
        "id": "I8eWhYFdGWNR",
        "outputId": "a37594b6-30f1-4db3-e856-393252f21087"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.Markdown object>",
            "text/markdown": "### CONTEXT\nCheese is the best food.\n\n### QUESTION\nWhat is the best food?\n\n### ANSWER\ncheese</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = \"Cheese is the best food.\"\n",
        "question = \"How far away is the Moon from the Earth?\"\n",
        "\n",
        "make_inference(context, question)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-19T09:34:53.363933Z",
          "iopub.execute_input": "2024-02-19T09:34:53.364636Z",
          "iopub.status.idle": "2024-02-19T09:34:58.846807Z",
          "shell.execute_reply.started": "2024-02-19T09:34:53.364587Z",
          "shell.execute_reply": "2024-02-19T09:34:58.845860Z"
        },
        "trusted": true,
        "id": "Ik6wjnc3GWNR",
        "outputId": "638be98d-8c78-4819-c9eb-801827a1520e"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.Markdown object>",
            "text/markdown": "### CONTEXT\nCheese is the best food.\n\n### QUESTION\nHow far away is the Moon from the Earth?\n\n### ANSWER\nCannot Find Answer</s>\n\n### ANSWER</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = \"The Moon orbits Earth at an average distance of 384,400 km (238,900 mi), or about 30 times Earth's diameter. Its gravitational influence is the main driver of Earth's tides and very slowly lengthens Earth's day. The Moon's orbit around Earth has a sidereal period of 27.3 days. During each synodic period of 29.5 days, the amount of visible surface illuminated by the Sun varies from none up to 100%, resulting in lunar phases that form the basis for the months of a lunar calendar. The Moon is tidally locked to Earth, which means that the length of a full rotation of the Moon on its own axis causes its same side (the near side) to always face Earth, and the somewhat longer lunar day is the same as the synodic period. However, 59% of the total lunar surface can be seen from Earth through cyclical shifts in perspective known as libration.\"\n",
        "question = \"At what distance does the Moon orbit the Earth?\"\n",
        "\n",
        "make_inference(context, question)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-02-19T09:35:27.668570Z",
          "iopub.execute_input": "2024-02-19T09:35:27.668925Z",
          "iopub.status.idle": "2024-02-19T09:35:33.168812Z",
          "shell.execute_reply.started": "2024-02-19T09:35:27.668898Z",
          "shell.execute_reply": "2024-02-19T09:35:33.167648Z"
        },
        "trusted": true,
        "id": "OOaOzAzpGWNR",
        "outputId": "8952fb78-763d-4ee3-de73-da45ea653df7"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.Markdown object>",
            "text/markdown": "### CONTEXT\nThe Moon orbits Earth at an average distance of 384,400 km (238,900 mi), or about 30 times Earth's diameter. Its gravitational influence is the main driver of Earth's tides and very slowly lengthens Earth's day. The Moon's orbit around Earth has a sidereal period of 27.3 days. During each synodic period of 29.5 days, the amount of visible surface illuminated by the Sun varies from none up to 100%, resulting in lunar phases that form the basis for the months of a lunar calendar. The Moon is tidally locked to Earth, which means that the length of a full rotation of the Moon on its own axis causes its same side (the near side) to always face Earth, and the somewhat longer lunar day is the same as the synodic period. However, 59% of the total lunar surface can be seen from Earth through cyclical shifts in perspective known as libration.\n\n### QUESTION\nAt what distance does the Moon orbit the Earth?\n\n### ANSWER\nCannot Find Answer</s>\n\n### ANSWER</s>\n\n### ANSWER</s>\n\n### ANSWER</s>\n\n### ANSWER</s>\n\n### ANSWER</s>\n\n### ANSWER</s>\n\n### ANSWER</s>\n\n### ANSWER</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### It looks like the GPT2 model is not doing a gread job in giving the write answer. We have trained the model on less data and also the model is smaller."
      ],
      "metadata": {
        "id": "yevXdp0MGWNS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wandb_key=  c891ce08bb56081ecfa97de464a131634657ac13\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "gRYAXrJcxUyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "LQWFobjCwmKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xzK0aGk1GWNS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}